{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badaca7f-1fc3-4f9e-9d68-3aa37603447d",
   "metadata": {},
   "source": [
    "自动文摘的目的是通过对原文本进行压缩、提炼，为用户供简明扼要的文字描述。自动文摘是一个信息压缩过程，将输入的一篇或多篇文档内容总结为一段简要描述，该过程不可避免有信息损失，但是要求保留尽可能多的重要信息，自动文摘也是自然语言生成领域中一个重要任务。\n",
    "下面我们以文本摘要任务为例，展示孟子预训练模型在下游任务上微调的流程，整体流程可以分为4部分：\n",
    "\n",
    "- 数据加载\n",
    "- 数据预处理\n",
    "- 模型训练\n",
    "- 模型推理\n",
    "- 评测\n",
    "\n",
    "下面我们以中文科学文献数据（CSL）文本摘要数据为例进行演示，数据下载地址：https://github.com/CLUEbenchmark/CLGE\n",
    "\n",
    "下载的原始数据：训练集(3,000)，验证集(500)，测试集(500)，但测试集没有摘要标注结果，所以这里我们简单地把验证集当作测试集，从训练集中划出500条作为开发集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318f008-84ce-4d04-886d-a91ca1cb6b15",
   "metadata": {},
   "source": [
    "## 依赖环境\n",
    "代码使用以下环境运行\n",
    "- transformers==4.12.5\n",
    "- sentencepiece==0.1.95\n",
    "- rouge==1.0.1\n",
    "- nltk==3.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9bdb4-e9b9-40e6-901e-e16a12fed49e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 数据加载\n",
    "\n",
    "CSL数据以json的形式存储，通过如下方式可以将数据加载进内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f0f96e-5856-47e5-96e4-505a51df983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878a541b-a822-429b-a2a8-9a0b775ac51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading...: 100%|██████████| 3000/3000 [00:00<00:00, 207013.67it/s]\n",
      "Reading...: 100%|██████████| 500/500 [00:00<00:00, 221826.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_json(input_file: str) -> list:\n",
    "    '''\n",
    "    读取json文件，每行是一个json字段\n",
    "\n",
    "    Args:\n",
    "        input_file:文件名\n",
    "\n",
    "    Returns:\n",
    "        lines\n",
    "    '''\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return list(map(json.loads, tqdm(lines, desc='Reading...')))\n",
    "\n",
    "trainset = read_json(\"csl/csl_title_train.json\") \n",
    "test = read_json(\"csl/csl_title_dev.json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f545e-ffbc-43f7-bdfc-204408194c83",
   "metadata": {},
   "source": [
    "下面展示数据集的具体信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e04393-9e66-42dc-b707-b31e1a3789f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：2500个训练样本\n",
      "开发集大小：500个训练样本\n",
      "每个训练样本的原始格式如下：\n",
      " {'id': 2364, 'title': '基于语义规则的Web服务发现方法', 'abst': '语义Web服务发现问题研究的核心内容是服务描述与对应的服务发现方法。服务描述分为服务请求描述与服务发布描述,但目前的服务发现方法,并未将请求描述与发布描述分开,以比对服务请求描述与服务发布描述中对应部分作为匹配依据,导致服务请求描述构建困难以及发现结果不够理想。提出以语义规则刻画服务请求描述,以本体构建服务发布描述,进行有效的以语义规则驱动的Web服务发现。对语义规则添加影响因子使得服务匹配精度可以通过匹配度来度量,并按照给定的调节系数来决定最终匹配是否成功。最后以OWL-STCV2测试服务集合进行了对比实验,证实该方法有效地提高了查全率与查准率高,特别是Top-k查准率。'}\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(trainset)\n",
    "train = trainset[:2500]\n",
    "dev = trainset[2500:]\n",
    "print('训练集大小：%d个训练样本'%(len(train)))\n",
    "print('开发集大小：%d个训练样本'%(len(dev)))\n",
    "print('每个训练样本的原始格式如下：\\n',train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc3d96-7cc3-4a06-97cd-9e59a08d4035",
   "metadata": {},
   "source": [
    "### 可以看出每条原始数据包含3个字段，分别是id，title，abst，其中id是唯一标识，abst是文本摘要任务的输入，title是文本摘要任务的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38e107-447d-4476-90a8-b1ee809c75dd",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4437e1d-85ab-46f3-aa99-eb2e6d1e6447",
   "metadata": {},
   "source": [
    "数据预处理的目的是将原始数据处理为模型可以接受的输入形式，相当于在原始数据和模型输入之间建立管道。\n",
    "模型输入，可接受的字段为input_ids、labels，其中input_ids为输入文本的tokenized表示，可以直接通过transformers提供的Tokenizer进行转换；labels为模型期望输出文本的tokenized表示。\n",
    "通过定义DataCollatorForSeq2Seq数据预处理类，将其传递给data_collator完成上述流程，数据预处理代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af4bb983-ebaf-4df4-ae42-1892be31bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Langboat/mengzi-t5-base\" # huggingface下载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83eec31-4ee5-471c-82be-752cc89d2c30",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 加载预训练模型，包括分词器tokenizer和model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af548e66-c49d-41d9-842b-0958b54183b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mengzi_tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43d696d-a974-41dd-a31d-7b8928754d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mengzi_model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9290477-3a9c-4264-9566-9f6df5663a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset:\n",
    "    def __init__(self, data):\n",
    "        self.datas = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.datas[index]\n",
    "\n",
    "class DataCollatorForSeq2Seq:\n",
    "    def __init__(self, tokenizer, padding: bool = True, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.model = model\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        features = self.collator_fn(batch)\n",
    "        return features\n",
    "\n",
    "\n",
    "    def preprocess(self, item):\n",
    "        source = item[\"abst\"]\n",
    "        target = item[\"title\"]\n",
    "        return source, target\n",
    "\n",
    "    def collator_fn(self, batch):\n",
    "        results = map(self.preprocess, batch)\n",
    "        inputs, targets = zip(*results)\n",
    "\n",
    "        input_tensor = self.tokenizer(inputs,\n",
    "                                      truncation=True,\n",
    "                                      padding=True,\n",
    "                                      max_length=self.max_length,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      )\n",
    "\n",
    "        target_tensor = self.tokenizer(targets,\n",
    "                                       truncation=True,\n",
    "                                       padding=True,\n",
    "                                       max_length=self.max_length,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       )\n",
    "\n",
    "        input_tensor[\"labels\"] = target_tensor[\"input_ids\"]\n",
    "\n",
    "        if \"token_type_ids\" in input_tensor:\n",
    "            del input_tensor[\"token_type_ids\"]\n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e12554a-00dc-4d0a-a874-06711381cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Seq2SeqDataset(train)\n",
    "devset = Seq2SeqDataset(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b48f1d71-8a04-4386-bc43-c1ac1b7db6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(Mengzi_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccbf5a-8206-4eaf-895f-2f0492c91df2",
   "metadata": {},
   "source": [
    "## 3. 模型训练\n",
    "\n",
    "训练模型前需要指定模型训练的超参数，包括训练的轮数、学习率和学习率管理策略等等：可以通过实例化TrainingArguments类来，并将其传递给Trainer来传入这些超参数。\n",
    "然后通过huggingface定义的trainer.train()方法来进行训练。\n",
    "训练完成后通过trainer.save_model()方法来保存最佳模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0449453c-f27a-4ab8-944f-f99d61c024eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"test\" # 模型checkpoint的保存目录\n",
    "training_args = TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        logging_steps=10,\n",
    "        #fp16=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=1e-5,\n",
    "        #warmup_steps=100,\n",
    "        output_dir=\"test\",\n",
    "        save_total_limit=5,\n",
    "        lr_scheduler_type='constant',\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "351a0034-eda7-4664-a180-1a9ea67beba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarining Arguments ...\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=100,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=test/runs/Nov01_11-42-16_JX-ZY-GPU12,\n",
      "logging_first_step=False,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "output_dir=test,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=test,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=test,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 939\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33megyang\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">test</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/egyang/huggingface\" target=\"_blank\">https://wandb.ai/egyang/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/egyang/huggingface/runs/2oyt4ccp\" target=\"_blank\">https://wandb.ai/egyang/huggingface/runs/2oyt4ccp</a><br/>\n",
       "                Run data is saved locally in <code>/nfs/users/yangerguang/chuangxin/summary/wandb/run-20211101_114247-2oyt4ccp</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 04:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.781900</td>\n",
       "      <td>1.654125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.669000</td>\n",
       "      <td>1.454569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.399100</td>\n",
       "      <td>1.426801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.294300</td>\n",
       "      <td>1.377249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.362800</td>\n",
       "      <td>1.363519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.497200</td>\n",
       "      <td>1.342906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.297500</td>\n",
       "      <td>1.321924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.294700</td>\n",
       "      <td>1.321687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.111900</td>\n",
       "      <td>1.306872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-100\n",
      "Configuration saved in test/checkpoint-100/config.json\n",
      "Model weights saved in test/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-100/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-100/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-200\n",
      "Configuration saved in test/checkpoint-200/config.json\n",
      "Model weights saved in test/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-200/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-200/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-300\n",
      "Configuration saved in test/checkpoint-300/config.json\n",
      "Model weights saved in test/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-300/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-300/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-400\n",
      "Configuration saved in test/checkpoint-400/config.json\n",
      "Model weights saved in test/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-400/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-400/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-500\n",
      "Configuration saved in test/checkpoint-500/config.json\n",
      "Model weights saved in test/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-500/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-600\n",
      "Configuration saved in test/checkpoint-600/config.json\n",
      "Model weights saved in test/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-600/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-600/spiece.model\n",
      "Deleting older checkpoint [test/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-700\n",
      "Configuration saved in test/checkpoint-700/config.json\n",
      "Model weights saved in test/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-700/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-700/spiece.model\n",
      "Deleting older checkpoint [test/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-800\n",
      "Configuration saved in test/checkpoint-800/config.json\n",
      "Model weights saved in test/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-800/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-800/spiece.model\n",
      "Deleting older checkpoint [test/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test/checkpoint-900\n",
      "Configuration saved in test/checkpoint-900/config.json\n",
      "Model weights saved in test/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in test/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in test/checkpoint-900/special_tokens_map.json\n",
      "Copy vocab file to test/checkpoint-900/spiece.model\n",
      "Deleting older checkpoint [test/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from test/checkpoint-900 (score: 1.306871771812439).\n",
      "Saving model checkpoint to test/best\n",
      "Configuration saved in test/best/config.json\n",
      "Model weights saved in test/best/pytorch_model.bin\n",
      "tokenizer config file saved in test/best/tokenizer_config.json\n",
      "Special tokens file saved in test/best/special_tokens_map.json\n",
      "Copy vocab file to test/best/spiece.model\n"
     ]
    }
   ],
   "source": [
    "print('Tarining Arguments ...')\n",
    "print(training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    tokenizer=Mengzi_tokenizer,\n",
    "    model=Mengzi_model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=devset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"test/best\") # 保存最好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef383301-4332-4092-840e-9899798fa98a",
   "metadata": {},
   "source": [
    "## 4. 模型推理\n",
    "\n",
    "最佳模型保存在了\"test/best\"位置，我们可以加载最佳模型并利用其进行摘要生成。\n",
    "下面是我们利用模型进行推理的一种实现方式，将希望简化的文本tokenized后传入模型，得到经过tokenizer解码后即可获得摘要后的文本。当然，读者也可以利用自己熟悉的方式进行生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d29708-29f3-45e3-9d13-e93d3ab24ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(items):\n",
    "    inputs = []\n",
    "    titles = []\n",
    "    for item in items:\n",
    "        inputs.append(item[\"abst\"])\n",
    "        titles.append(item[\"title\"])\n",
    "    return inputs, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f06189-1ec3-4b8b-be41-f27bebb5e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = \"test/best\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(best_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(best_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a4834d-ae9e-40c9-97f6-373585ece54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sources, batch_size=8):\n",
    "    _model = model.eval() # 将模型转换为预测模式，使模型内容的droput失效。\n",
    "    \n",
    "    kwargs = {\"num_beams\":4}\n",
    "    \n",
    "    outputs = []\n",
    "    for start in tqdm(range(0, len(sources), batch_size)):\n",
    "        batch = sources[start:start+batch_size]\n",
    "        \n",
    "        input_tensor = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).input_ids.cuda()\n",
    "        \n",
    "        outputs.extend(model.generate(input_ids=input_tensor, **kwargs))\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449b1a65-18ca-4e78-9b1e-25df3e7677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, refs = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668efe77-fa52-49f7-9946-c3d7a6ced3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'抽象了一种基于中心的战术应用场景与业务,并将网络编码技术应用于此类场景的实时数据多播业务中。在分析基于中心网络与Many-to-all业务模式特性的基础上,提出了仅在中心节点进行编码操作的传输策略以及相应的贪心算法。分析了网络编码多播策略的理论增益上界,仿真试验表明该贪心算法能够获得与理论相近的性能增益。最后的分析与仿真试验表明,在这种有中心网络的实时数据多播应用中,所提出的多播策略的实时性能要明显优于传统传输策略。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc14f77c-53ae-4ed2-9bab-9bc497a9b01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'网络编码在实时战术数据多播中的应用'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95aafa6d-45e7-4b04-a56a-403ac45529bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:25<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "generations = predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca784168-6500-4390-9e88-5b050f703ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'基于中心网络的实时数据多播应用'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc461586-8d0e-42f8-8a95-a514cadafc8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. 生成结果的评测\n",
    "\n",
    "采用自动文摘任务上常用的自动评测指标Rouge-1, Rouge-2, Rouge-L对生成文本的质量进行评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d4f2be-b4a4-404a-9717-ad6923a4215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to help students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teacher or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4153cc33-6496-4a6d-888f-9b4e20d61c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.4583333333333333,\n",
       "   'p': 0.6285714285714286,\n",
       "   'f': 0.5301204770503702},\n",
       "  'rouge-2': {'r': 0.21739130434782608, 'p': 0.375, 'f': 0.2752293531520916},\n",
       "  'rouge-l': {'r': 0.4166666666666667,\n",
       "   'p': 0.5714285714285714,\n",
       "   'f': 0.4819277059660328}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc295a-63c6-41be-a552-4e492c6295c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9e06f0-36c0-412e-8355-dd5257d74088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge_score(candidate, reference):\n",
    "    text1 = \" \".join(list(candidate))\n",
    "    text2 = \" \".join(list(reference))\n",
    "    score = rouge.get_scores(text1, text2)\n",
    "    return score\n",
    "\n",
    "def compute_rouge(preds, refs):\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    R_L=[]\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        scores = rouge_score(pred, ref)\n",
    "        r1.append(scores[0][\"rouge-1\"][\"f\"])\n",
    "        r2.append(scores[0][\"rouge-2\"][\"f\"])\n",
    "        R_L.append(scores[0][\"rouge-l\"][\"f\"])\n",
    "    return sum(r1)/len(r1), sum(r2)/len(r2), sum(R_L)/len(R_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e34b7f4-04c6-4b43-85bf-ac3461be3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_1, R_2, R_L = compute_rouge(generations, refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
